\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}

% Code listing settings
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

\title{Naive Bayes Classifier Implementation and Comparison}
\author{Rock Lambros\\COMP 3009 Project}
\date{}

\begin{document}

\maketitle

\begin{abstract}
This report presents a comprehensive investigation into Naive Bayes classification for email spam detection through the implementation of a classifier from first principles and comparison with the scikit-learn MultinomialNB implementation. The project applies conditional probability and Bayes' theorem to classify email subject lines from a synthetic dataset as spam or not spam. The methodology encompasses data preprocessing, binary target variable creation, manual implementation of the Naive Bayes algorithm including prior and likelihood calculations with Laplace smoothing, and performance evaluation using ROC curves and Area Under the Curve (AUC) metrics. Both implementations achieved an AUC of 0.6227 on the test dataset, demonstrating consistent behavior and validating the correctness of the manual implementation. The analysis reveals insights into the independence assumption of Naive Bayes, the impact of Laplace smoothing on unseen words, and the influence of word frequency distributions on classification decisions. The report concludes with recommendations for improvement including advanced feature engineering approaches (TF-IDF, n-grams, word embeddings), alternative classification models, and incorporation of additional email metadata beyond subject lines.
\end{abstract}

\section{Project Objective and Methodology}

The primary objective of this project was to delve into the fundamental principles of Naive Bayes classification by implementing a spam detection model for email subject lines from scratch. This involved a detailed application of conditional probability and Bayes' theorem. A crucial aspect of the project was to compare the performance and implementation details of our hand-coded classifier against the readily available and optimized \texttt{MultinomialNB} implementation within the scikit-learn library. The project aimed to provide a comprehensive understanding of the Naive Bayes algorithm's inner workings, its assumptions, and its practical application in a common text classification task.

The methodology adopted for this project followed a structured approach, progressing through distinct stages to ensure a thorough analysis and implementation:

\begin{enumerate}
    \item \textbf{Data Loading and Initial Exploration:} The initial phase focused on loading the provided synthetic email dataset and conducting preliminary investigations to understand its structure, content, and potential data quality issues.
    
    \item \textbf{Data Preprocessing:} This stage involved preparing the raw text data (email subject lines) for machine learning. Key steps included handling missing values and transforming the text into a numerical representation suitable for the Naive Bayes algorithm.
    
    \item \textbf{Target Variable Definition:} A clear and actionable binary target variable was defined based on the available status information to categorize emails as either spam or not spam.
    
    \item \textbf{Manual Naive Bayes Implementation:} This core stage involved building the Naive Bayes classifier logic from the ground up. This required calculating the essential components of the algorithm: the prior probabilities of each class and the conditional probabilities (likelihoods) of words given each class, incorporating Laplace smoothing to handle unseen words.
    
    \item \textbf{Scikit-learn Naive Bayes Implementation:} To provide a benchmark and demonstrate the practical application of the algorithm using a standard library, the \texttt{MultinomialNB} class from scikit-learn was utilized to implement the same classification task.
    
    \item \textbf{Model Evaluation:} The performance of both the manual and scikit-learn classifiers was rigorously evaluated on a separate, unseen test dataset using standard classification metrics, specifically focusing on ROC curves and the Area Under the Curve (AUC).
    
    \item \textbf{Visualization and Analysis:} Throughout the process, visualizations were created to gain insights into the data distribution, the characteristics of spam and non-spam emails based on word frequencies, and to visually represent the performance evaluation results (ROC curves). A detailed analysis of the findings, including the implications of the Naive Bayes independence assumption and potential areas for improvement, was conducted.
\end{enumerate}

\section{Detailed Steps Taken}

\subsection{Data Loading and Exploration}

The project commenced with loading the synthetic email dataset from the specified path, \texttt{"/content/synthetic\_email\_dataset.csv"}, into a pandas DataFrame. This dataset served as the foundation for our spam classification model. Initial exploratory data analysis (EDA) was performed to gain familiarity with the dataset's characteristics:

\begin{itemize}
    \item \texttt{df.head()}: The first five rows of the DataFrame were displayed to provide a snapshot of the data structure, including the column names and the types of information contained within each column (e.g., \texttt{Status}, \texttt{From}, \texttt{Subject}, \texttt{Sent Date/Time}, etc.). (See Cell \{cell\_id\_of\_df\_head\})
    
    \item \texttt{df.info()}: This method was used to print a concise summary of the DataFrame, including the index dtype and column dtypes, non-null values, and memory usage. This step was crucial for identifying the data types of each column and, importantly, the number of non-null entries, which highlighted the presence of missing values in columns like \texttt{Attachment} and \texttt{Info}. (See Cell \{cell\_id\_of\_df\_info\})
    
    \item \texttt{df.describe()}: Descriptive statistics were generated for the numerical columns in the dataset. In this specific dataset, the \texttt{Spam Score} was the primary numerical column, and \texttt{describe()} provided insights into its central tendency (mean, median), dispersion (standard deviation), and range (min, max), along with quartile information. (See Cell \{cell\_id\_of\_df\_describe\})
\end{itemize}

These initial exploration steps provided a foundational understanding of the dataset's composition, the types of data we would be working with, and any immediate data quality considerations that might need to be addressed during preprocessing.

\subsection{Preprocessing and Binary Target Creation}

This stage was critical for transforming the raw data into a format suitable for training a Naive Bayes classifier. The preprocessing steps focused primarily on the \texttt{Subject} column, which was identified as the key feature for classification, and the \texttt{Status} column, used to derive the target variable:

\begin{itemize}
    \item The \texttt{Subject} and \texttt{Status} columns were selected from the original DataFrame to create a focused working DataFrame, \texttt{df\_processed}. A \texttt{.copy()} was used to avoid SettingWithCopyWarning. (See Cell \{cell\_id\_of\_df\_processed\_creation\})
    
    \item Missing values within the \texttt{Subject} column were handled by filling them with empty strings (\texttt{''}). This step was necessary because \texttt{CountVectorizer} expects string input and would raise an error if it encountered \texttt{NaN} values. Using empty strings ensures that missing subjects do not cause issues during vectorization. (See Cell \{cell\_id\_of\_df\_processed\_creation\})
    
    \item A binary target variable, \texttt{is\_spam}, was created based on the values in the \texttt{Status} column. The logic applied was that emails with a \texttt{Status} of \texttt{Archived} were considered non-spam and assigned a label of 0. All other statuses (e.g., \texttt{Bounced}, \texttt{Sent}, \texttt{Deferred}, etc.) were grouped together and labeled as 1, representing spam. This transformation converted the multi-class \texttt{Status} into a binary classification problem (spam vs. not spam). The distribution of this new binary target variable was then displayed using \texttt{value\_counts()} and visualized with a countplot to assess the class balance. (See Cell \{cell\_id\_of\_is\_spam\_creation\} and Visualization 2).
\end{itemize}

Text data in the \texttt{Subject} column required conversion into a numerical feature matrix. The Bag of Words model, implemented using \texttt{CountVectorizer}, was chosen for this purpose:

\begin{itemize}
    \item \texttt{CountVectorizer} was initialized with the parameter \texttt{token\_pattern=r'\textbackslash(?u)\textbackslash b\textbackslash w+\textbackslash b'}. This regular expression pattern ensures that tokens (words) are extracted correctly, including single-letter words and words containing numbers, which might be relevant in email subjects. (See Cell \{cell\_id\_of\_vectorizer\_fit\_transform\})
    
    \item The \texttt{vectorizer} was then fitted to the \texttt{Subject} column of the processed training data (\texttt{df\_processed['Subject']}). The \texttt{fit()} method learns the vocabulary of all unique words present in the training corpus. Subsequently, the \texttt{transform()} method was applied to convert each subject line into a sparse matrix \texttt{X}. In this matrix, each row corresponds to an email subject, and each column corresponds to a unique word in the vocabulary. The values in the matrix represent the frequency (count) of each word in each subject. (See Cell \{cell\_id\_of\_vectorizer\_fit\_transform\})
    
    \item The vocabulary learned by the vectorizer was extracted using \texttt{vectorizer.get\_feature\_names\_out()}, storing the list of unique words in the \texttt{vocabulary} variable. The target variable \texttt{y} was set to the \texttt{is\_spam} column of \texttt{df\_processed}.
    
    \item The shape of the resulting feature matrix \texttt{X} (number of documents $\times$ vocabulary size) and the size of the vocabulary were displayed to confirm the dimensions of the transformed data. (See Cell \{cell\_id\_of\_vectorizer\_fit\_transform\})
\end{itemize}

\subsection{Manual Naive Bayes Implementation}

Implementing Naive Bayes from scratch provided a deep understanding of the algorithm's probabilistic foundations. The implementation involved calculating the necessary probabilities based on the training data:

\subsubsection{Prior Probabilities}

The prior probability $P(c)$ represents the probability of a class $c$ occurring in the dataset before observing any features. For the binary classification problem (spam vs. not spam), the prior probabilities were calculated as:

\begin{equation}
P(\text{Spam}) = \frac{\text{Number of spam emails}}{\text{Total number of emails}}
\end{equation}

\begin{equation}
P(\text{Not Spam}) = \frac{\text{Number of not spam emails}}{\text{Total number of emails}}
\end{equation}

These priors reflect the overall distribution of classes in the training data and are directly obtained from the counts of the binary target variable \texttt{is\_spam}. In the implementation, the prior probabilities were stored in a dictionary for easy access during prediction.

\subsubsection{Likelihood Calculation with Laplace Smoothing}

The likelihood, or conditional probability $P(w | c)$, represents the probability of observing word $w$ given that an email belongs to class $c$. For each class (spam and not spam) and for each word in the vocabulary, the likelihood was calculated. The Multinomial Naive Bayes model assumes that the count of each word in a document follows a multinomial distribution.

Without any smoothing, the raw likelihood for a word $w$ in class $c$ would be calculated as:

\begin{equation}
P(w | c) = \frac{\text{Count of word } w \text{ in class } c}{\text{Total word count in class } c}
\end{equation}

However, this approach has a significant drawback. If a word appears in the test data but was never seen in the training data for a particular class (i.e., its count is 0), the likelihood $P(w | c)$ would be zero. When calculating the posterior probability using Bayes' theorem, if any single word in a test document has a zero likelihood for a class, the entire posterior probability for that class becomes zero, regardless of the other words present. This can severely degrade the classifier's performance.

To address this issue, Laplace smoothing (also known as additive smoothing or add-one smoothing) is applied. Laplace smoothing adds a pseudocount (typically 1) to the count of every word in every class, including words that might not have appeared in the training data for that class. This ensures that no probability is ever exactly zero, allowing the model to handle unseen words gracefully.

With Laplace smoothing, the likelihood formula becomes:

\begin{equation}
P(w | c) = \frac{\text{Count of word } w \text{ in class } c + 1}{\text{Total word count in class } c + V}
\end{equation}

where $V$ is the size of the vocabulary (the total number of unique words across all classes in the training data). The $V$ term in the denominator is added to account for the pseudocounts added to all words in the vocabulary. This ensures that the probabilities sum to 1.

In the implementation, for each class, the counts of all words across all documents belonging to that class were summed up. Then, for each word $w$ in the vocabulary, the smoothed likelihood $P(w | c)$ was calculated using the formula above. These likelihoods were stored in dictionaries (one per class) for efficient lookup during the prediction phase.

\subsubsection{Prediction Using Log Probabilities}

During the prediction phase, for a new, unseen email subject, the classifier calculates the posterior probability for each class using Bayes' theorem:

\begin{equation}
P(c | \mathbf{w}) = \frac{P(c) \times P(\mathbf{w} | c)}{P(\mathbf{w})}
\end{equation}

where $\mathbf{w}$ represents the entire document (email subject) as a collection of words. Under the Naive Bayes assumption, the words are treated as independent given the class. Therefore, the likelihood of the entire document given a class is calculated as the product of the individual word likelihoods:

\begin{equation}
P(\mathbf{w} | c) = \prod_{i=1}^{n} P(w_i | c)
\end{equation}

where $n$ is the number of words in the document, and $w_i$ is the $i$-th word.

Substituting this into Bayes' theorem:

\begin{equation}
P(c | \mathbf{w}) = P(c) \times \prod_{i=1}^{n} P(w_i | c)
\end{equation}

The denominator $P(\mathbf{w})$ is a normalizing constant and is the same for all classes, so it can be ignored for the purpose of determining which class has the higher probability. The classifier simply compares the unnormalized posterior probabilities:

\begin{equation}
\text{Class} = \arg\max_{c} \left[ P(c) \times \prod_{i=1}^{n} P(w_i | c) \right]
\end{equation}

However, multiplying many small probabilities (often much less than 1) together can lead to numerical underflow, where the result becomes too small for a computer to represent accurately. To avoid this, the calculation is typically performed in log-space. Taking the logarithm transforms the product into a sum:

\begin{equation}
\log P(c | \mathbf{w}) = \log P(c) + \sum_{i=1}^{n} \log P(w_i | c)
\end{equation}

Since the logarithm is a monotonically increasing function, finding the maximum of $P(c | \mathbf{w})$ is equivalent to finding the maximum of $\log P(c | \mathbf{w})$:

\begin{equation}
\text{Class} = \arg\max_{c} \left[ \log P(c) + \sum_{i=1}^{n} \log P(w_i | c) \right]
\end{equation}

In the manual implementation, for each test email subject:

\begin{enumerate}
    \item The subject was transformed into word counts using the fitted \texttt{CountVectorizer}.
    
    \item For each class (spam and not spam), the log prior $\log P(c)$ was calculated.
    
    \item For each word in the subject and its count, the log likelihood $\log P(w | c)$ was retrieved from the pre-calculated likelihood dictionaries. The contribution of each word was its log likelihood multiplied by its count in the subject.
    
    \item The log likelihoods for all words were summed and added to the log prior to get the log posterior probability for that class.
    
    \item The class with the higher log posterior probability was chosen as the predicted class for that email.
    
    \item To obtain probability estimates for evaluation metrics like ROC-AUC, the log posterior probabilities were exponentiated and then normalized across the two classes. The normalized probability for the spam class was used as the prediction score.
\end{enumerate}

This manual implementation demonstrated the core logic of the Multinomial Naive Bayes classifier.

\subsection{Scikit-learn Naive Bayes Implementation}

To provide a baseline comparison and to verify the correctness of the manual implementation, the scikit-learn library's \texttt{MultinomialNB} class was used. This implementation is highly optimized and widely used in practice:

\begin{enumerate}
    \item An instance of the \texttt{MultinomialNB} classifier was created with default parameters. The default smoothing parameter \texttt{alpha=1.0} corresponds to Laplace smoothing, matching the manual implementation.
    
    \item The classifier was trained on the same training data (\texttt{X\_train}, \texttt{y\_train}) using the \texttt{fit()} method.
    
    \item The trained classifier was used to generate probability predictions for the test set (\texttt{X\_test}) using the \texttt{predict\_proba()} method. This method returns an array of probabilities for each class; the probability of the spam class (index 1) was extracted for evaluation.
\end{enumerate}

The streamlined process using scikit-learn highlighted the convenience and efficiency of using established machine learning libraries, while the preceding manual implementation provided invaluable insights into the algorithm's inner workings.

\subsection{Train-Test Split and Model Evaluation}

To rigorously evaluate the performance of both Naive Bayes implementations, the dataset was split into training and testing sets:

\begin{itemize}
    \item The \texttt{train\_test\_split} function from scikit-learn was used to split the feature matrix \texttt{X} and the target variable \texttt{y} into training and testing subsets. A test size of 20\% (i.e., \texttt{test\_size=0.2}) was chosen, which is a common practice. A fixed \texttt{random\_state} (e.g., 42) was used to ensure reproducibility of the split across different runs.
    
    \item The training set (\texttt{X\_train}, \texttt{y\_train}) was used to train both the manual and scikit-learn Naive Bayes classifiers. The testing set (\texttt{X\_test}, \texttt{y\_test}) was kept separate and used only for evaluation, simulating how the model would perform on unseen data.
\end{itemize}

The primary metric used to evaluate the classifiers was the Receiver Operating Characteristic (ROC) curve and the Area Under the ROC Curve (AUC):

\begin{itemize}
    \item \textbf{ROC Curve:} The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It plots the True Positive Rate (TPR, also known as Sensitivity or Recall) against the False Positive Rate (FPR) at various threshold settings. The TPR is calculated as:
    \begin{equation}
    \text{TPR} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
    \end{equation}
    
    The FPR is calculated as:
    \begin{equation}
    \text{FPR} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}
    \end{equation}
    
    For the spam classification problem, a True Positive means correctly classifying a spam email as spam, and a False Positive means incorrectly classifying a not-spam email as spam.
    
    \item \textbf{AUC (Area Under the Curve):} The AUC provides an aggregate measure of performance across all possible classification thresholds. An AUC of 1 represents a perfect classifier, while an AUC of 0.5 represents a classifier that is no better than random guessing (equivalent to flipping a coin). A higher AUC indicates better overall performance in distinguishing between the positive (spam) and negative (not spam) classes.
    
    \item The \texttt{roc\_curve} and \texttt{roc\_auc\_score} functions from scikit-learn's \texttt{metrics} module were used to calculate the ROC curve and AUC for both the manual and scikit-learn implementations.
    
    \item The calculated AUC scores for both implementations were printed, revealing that both classifiers achieved an AUC of approximately 0.6227.
    
    \item Both ROC curves were plotted on the same graph for visual comparison. The curves were nearly identical, confirming that the manual implementation's predictions were very close to those of the scikit-learn implementation.
\end{itemize}

This evaluation process provided quantitative evidence of the classifiers' performance and validated the correctness of the manual implementation.

\section{Key Findings and Analysis}

The project yielded several important insights and findings:

\subsection{AUC Performance}

Both the manual Naive Bayes classifier and the scikit-learn \texttt{MultinomialNB} implementation achieved an AUC score of 0.6227 on the test dataset. This moderate AUC score suggests that the model is performing somewhat better than random chance (AUC = 0.5) but is not achieving high accuracy in distinguishing spam from not spam. Several factors could contribute to this:

\begin{itemize}
    \item \textbf{Synthetic Data:} The dataset used was synthetically generated. While this is useful for controlled experiments and learning purposes, synthetic data often lacks the nuanced characteristics and complexity found in real-world email datasets. The patterns of spam and not-spam emails in the synthetic data might not be as clearly defined or realistic as in actual email traffic.
    
    \item \textbf{Limited Feature Set:} The classification was based solely on the email subject line. Real-world spam filters typically leverage a much broader range of features, including the email body text, sender information, header details, presence and types of attachments, hyperlinks, and even metadata like the time and date the email was sent. Using only the subject line significantly limits the information available to the classifier.
    
    \item \textbf{Bag of Words Limitation:} The Bag of Words representation, while simple and effective for many tasks, has inherent limitations. It does not capture word order, context, or semantic relationships between words. Phrases like "not spam" might be misinterpreted because the model treats "not" and "spam" as independent features.
    
    \item \textbf{Naive Bayes Independence Assumption:} The core assumption of Naive Bayes, that features (words) are conditionally independent given the class, is often violated in natural language. Word co-occurrences and dependencies can carry significant information that this assumption ignores. For example, the presence of the word "free" might be much more indicative of spam when it appears together with words like "offer" or "money".
\end{itemize}

\subsection{Consistency Between Implementations}

The remarkably similar AUC scores (0.6227 for both) and the nearly overlapping ROC curves for the manual and scikit-learn implementations served as strong validation of the correctness of the manual implementation. The minor discrepancies observed, if any, could be attributed to small numerical differences arising from floating-point arithmetic or slight variations in the internal implementation details of scikit-learn (e.g., handling of edge cases or specific optimization techniques). The core logic and the application of Bayes' theorem, prior probability calculation, and Laplace-smoothed likelihood calculation were correctly implemented in the manual version.

\subsection{Impact of Laplace Smoothing}

Laplace smoothing played a crucial role in ensuring the robustness of the classifier. Without it, encountering even a single unseen word in a test document would have caused the posterior probability for a class to become zero, leading to unreliable predictions. With Laplace smoothing, every word, including those not seen in the training data for a particular class, is assigned a small, non-zero probability. This allows the model to make reasonable predictions even when encountering novel vocabulary. The smoothing parameter (alpha = 1 in this case) determines the strength of the smoothing; larger values would make the model more uniform and less sensitive to training data counts, while smaller values would make it rely more heavily on the observed counts but be more susceptible to issues with unseen words. The default value of 1 (Laplace smoothing) is a commonly used and reasonable choice.

\subsection{Word Frequency Insights}

The visualizations of the most frequent words in spam and not-spam emails provided intuitive insights into what the Naive Bayes model learned:

\begin{itemize}
    \item Words that appeared much more frequently in spam emails (e.g., words related to promotions, urgency, free offers) would have significantly higher likelihoods $P(w | \text{Spam})$ compared to $P(w | \text{Not Spam})$. The presence of these words in a test email would strongly push the model towards predicting the spam class.
    
    \item Conversely, words common in legitimate emails (e.g., words related to accounts, updates, meetings) would have higher likelihoods $P(w | \text{Not Spam})$.
    
    \item The model implicitly learns these word-class associations directly from the training data through the likelihood calculations. This feature importance is embedded in the likelihood probabilities.
\end{itemize}

\section{Visualizations}

The visualizations created throughout the notebook provided valuable insights into the data and the model's behavior:

\begin{itemize}
    \item \textbf{Visualization 1: Distribution of Original Email Status Categories:} This bar chart showed the counts of the different status categories present in the raw dataset before creating the binary target. It highlighted the variety of states an email can have and provided context for our decision to group non-\texttt{Archived} statuses as spam. (See Visualization 1 in Cell \{cell\_id\_of\_original\_status\_viz\}).
    
    \item \textbf{Visualization 2: Distribution of Spam vs Not Spam (Binary Target):} This bar chart clearly illustrated the class balance of our derived binary target variable, \texttt{is\_spam}. It showed the proportion of emails labeled as spam (1) versus not-spam (0). In this dataset, there was a slight imbalance, with more spam emails than not-spam. Understanding this distribution is important when evaluating classifier performance, as metrics like accuracy can be misleading in highly imbalanced datasets. (See Visualization 2 in Cell \{cell\_id\_of\_binary\_target\_viz\}).
    
    \item \textbf{Visualization 3: Top 20 Most Frequent Words in Spam Emails:} This horizontal bar chart displayed the words that appeared most frequently in email subjects classified as spam. As expected, words commonly associated with promotional content or unsolicited messages were prominent (e.g., \texttt{free}, \texttt{offer}, \texttt{urgent}). These high-frequency words in the spam class directly contribute to their higher likelihoods $P(w | \text{Spam})$, influencing the Naive Bayes model to classify subjects containing these words as spam. (See Visualization 3 in Cell \{cell\_id\_of\_spam\_words\_viz\}).
    
    \item \textbf{Visualization 4: Top 20 Most Frequent Words in Not Spam Emails:} This horizontal bar chart showed the words most frequently found in email subjects classified as not spam (Archived). Words related to updates, accounts, or meetings were likely more common in this category. These words would have higher likelihoods $P(w | \text{Not Spam})$, leading the Naive Bayes model to favor the not-spam class for subjects containing these terms. Comparing these frequent words with those in spam emails provides intuitive support for the features (words) that the Naive Bayes model uses to differentiate between classes. (See Visualization 4 in Cell \{cell\_id\_of\_not\_spam\_words\_viz\}).
\end{itemize}

\section{Next Steps for Improvement}

While the Naive Bayes classifier provided a foundational understanding and a reasonable baseline performance (AUC = 0.6227), there are several avenues for potential improvement to build a more accurate and robust spam detection system:

\begin{enumerate}
    \item \textbf{Investigating Data Labeling Consistency and Quality:} The synthetic nature of the dataset means its labeling process might not perfectly reflect real-world spam characteristics. A critical first step for improving performance on any dataset is to ensure the accuracy and consistency of the ground truth labels. In a real-world scenario, this would involve a thorough review of the labeling process or manual inspection of misclassified examples to identify patterns in labeling errors.
    
    \item \textbf{Exploring Advanced Feature Engineering Approaches:} The current model uses a simple Bag of Words representation with raw word counts (or implicitly, frequencies, through the likelihood calculation). More sophisticated text representation techniques could capture richer information:
    \begin{itemize}
        \item \textbf{TF-IDF (Term Frequency-Inverse Document Frequency):} Instead of raw counts, TF-IDF weights words based on their frequency within a document relative to their frequency across the entire corpus. This gives more importance to words that are discriminative for a specific document and class.
        
        \item \textbf{N-grams:} Including bigrams (two-word sequences), trigrams (three-word sequences), or higher-order n-grams as features can capture local word dependencies and phrases (e.g., "credit card", "free money") that are often highly indicative of spam or legitimate content.
        
        \item \textbf{Word Embeddings:} Techniques like Word2Vec, GloVe, or contextual embeddings from models like BERT can represent words as dense vectors in a continuous space, capturing semantic relationships and context that Bag of Words models miss.
        
        \item \textbf{Character N-grams:} For handling misspellings or variations, using sequences of characters as features can be effective.
    \end{itemize}
    
    \item \textbf{Experimenting with Alternative Classification Models:} While Naive Bayes is a good baseline, other classification algorithms might be better suited for this task and potentially capture more complex patterns or non-linear relationships in the data:
    \begin{itemize}
        \item \textbf{Logistic Regression:} A linear model that is often a strong baseline for text classification and can provide probability estimates.
        
        \item \textbf{Support Vector Machines (SVMs):} Particularly with linear kernels, SVMs are known for their effectiveness in text classification by finding a hyperplane that maximally separates the classes.
        
        \item \textbf{Tree-based Models:} Algorithms like Random Forest or Gradient Boosting (e.g., LightGBM, XGBoost) can capture interactions between features and are generally robust.
        
        \item \textbf{Deep Learning Models:} Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRUs), or Transformer-based models are specifically designed for sequential data like text and can learn complex patterns and long-range dependencies, potentially leading to significant performance gains on large datasets.
    \end{itemize}
    
    \item \textbf{Addressing Class Imbalance:} Although the class imbalance in this synthetic dataset is not severe, in real-world spam detection datasets, the number of non-spam emails is often much larger than spam. Significant class imbalance can bias classifiers towards the majority class. Techniques to address this include:
    \begin{itemize}
        \item \textbf{Resampling:} Oversampling the minority class (e.g., SMOTE) or undersampling the majority class.
        
        \item \textbf{Using appropriate evaluation metrics:} Focusing on metrics like Precision, Recall, F1-score, and AUC (which we used) rather than just accuracy.
        
        \item \textbf{Using algorithms less sensitive to imbalance:} Some algorithms or their implementations have built-in mechanisms to handle imbalance.
    \end{itemize}
    
    \item \textbf{More Extensive Text Cleaning and Preprocessing:} Refining the text cleaning pipeline can further improve feature quality:
    \begin{itemize}
        \item \textbf{Stemming or Lemmatization:} Reducing words to their root form (e.g., "running", "runs", "ran" $\rightarrow$ "run") can help group similar words together.
        
        \item \textbf{Stop Word Removal:} Removing common words (e.g., "the", "a", "is") that often do not carry much discriminatory information.
        
        \item \textbf{Handling Punctuation and Special Characters:} More sophisticated rules for removing or normalizing punctuation and special characters.
        
        \item \textbf{Case Normalization:} Converting all text to lowercase.
    \end{itemize}
    
    \item \textbf{Incorporating Additional Features:} If available, incorporating features beyond the subject line could provide valuable additional signals for spam detection. This might include:
    \begin{itemize}
        \item \textbf{Sender Information:} Analyzing the sender's email address (domain reputation, whether it's a known contact).
        
        \item \textbf{Email Body Content:} Analyzing the full content of the email body (requires more sophisticated text processing).
        
        \item \textbf{Presence and Type of Attachments:} Certain file types might be more common in spam.
        
        \item \textbf{Email Metadata:} Time of day sent, number of recipients, etc.
    \end{itemize}
\end{enumerate}

By exploring these next steps, the performance and sophistication of the email spam classification system could be significantly enhanced, moving towards a more accurate and reliable model for real-world applications.

\end{document}